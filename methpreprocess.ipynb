{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36d0e736",
   "metadata": {},
   "source": [
    "# Install and import all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4a0747f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.4.1)\n",
      "Requirement already satisfied: h5py in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: mpmath in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: sklearn in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: pyensembl in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.9.4)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.22.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.31.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sklearn) (1.0.2)\n",
      "Requirement already satisfied: typechecks>=0.0.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyensembl) (0.1.0)\n",
      "Requirement already satisfied: datacache>=1.1.4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyensembl) (1.1.5)\n",
      "Requirement already satisfied: memoized-property>=1.0.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyensembl) (1.0.3)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyensembl) (1.16.0)\n",
      "Requirement already satisfied: gtfparse>=1.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyensembl) (1.2.1)\n",
      "Requirement already satisfied: serializable in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyensembl) (0.2.1)\n",
      "Requirement already satisfied: tinytimer in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyensembl) (0.0.0)\n",
      "Requirement already satisfied: appdirs>=1.4.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datacache>=1.1.4->pyensembl) (1.4.4)\n",
      "Requirement already satisfied: progressbar33>=2.4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datacache>=1.1.4->pyensembl) (2.4)\n",
      "Requirement already satisfied: requests>=2.5.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datacache>=1.1.4->pyensembl) (2.27.1)\n",
      "Requirement already satisfied: mock in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datacache>=1.1.4->pyensembl) (4.0.3)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sklearn) (1.8.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Requirement already satisfied: simplejson in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from serializable->pyensembl) (3.17.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.5.1->datacache>=1.1.4->pyensembl) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.5.1->datacache>=1.1.4->pyensembl) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.5.1->datacache>=1.1.4->pyensembl) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.5.1->datacache>=1.1.4->pyensembl) (3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas h5py joblib matplotlib mpmath sklearn pyensembl\n",
    "# !pip install combat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8edf0218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-06 16:29:06,371 - pyensembl.shell - INFO - Running 'install' for EnsemblRelease(release=104, species='homo_sapiens')\n",
      "2022-04-06 16:29:07,289 - pyensembl.sequence_data - INFO - Loaded sequence dictionary from C:\\Users\\HP\\AppData\\Local\\pyensembl\\GRCh38\\ensembl104\\pyensembl\\GRCh38\\ensembl104\\Cache\\Homo_sapiens.GRCh38.cdna.all.fa.gz.pickle\n",
      "2022-04-06 16:29:07,476 - pyensembl.sequence_data - INFO - Loaded sequence dictionary from C:\\Users\\HP\\AppData\\Local\\pyensembl\\GRCh38\\ensembl104\\pyensembl\\GRCh38\\ensembl104\\Cache\\Homo_sapiens.GRCh38.ncrna.fa.gz.pickle\n",
      "2022-04-06 16:29:07,669 - pyensembl.sequence_data - INFO - Loaded sequence dictionary from C:\\Users\\HP\\AppData\\Local\\pyensembl\\GRCh38\\ensembl104\\pyensembl\\GRCh38\\ensembl104\\Cache\\Homo_sapiens.GRCh38.pep.all.fa.gz.pickle\n"
     ]
    }
   ],
   "source": [
    "!pyensembl install --release 104 --species homo_sapiens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a82bdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, h5py, argparse, sys\n",
    "from collections import Counter\n",
    "from joblib import Parallel, delayed\n",
    "# from combat.pycombat import pycombat\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pyensembl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ff6393",
   "metadata": {},
   "source": [
    "# Define all functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bc32751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames(meth_raw_dir):\n",
    "    \"\"\"Extract all valid methylation files in dir.\n",
    "    \"\"\"\n",
    "    filenames_450 = []\n",
    "    filenames_850 = []\n",
    "    skipped_files = 0\n",
    "    for dname in os.listdir(meth_raw_dir):\n",
    "        sub_dirname = os.path.join(meth_raw_dir, dname)\n",
    "        if os.path.isdir(sub_dirname):\n",
    "            for fname in os.listdir(sub_dirname):\n",
    "                if fname.endswith('gdc_hg38.txt'):\n",
    "                        filenames_450.append(os.path.join(sub_dirname, fname))\n",
    "                elif fname.endswith('level3betas.txt'):\n",
    "                        filenames_850.append(os.path.join(sub_dirname, fname))\n",
    "                else:\n",
    "                        skipped_files += 1\n",
    "    print (\"Skipped {} arrays\".format(skipped_files))\n",
    "    print (\"Found {} 450k arrays and {} epic arrays\".format(len(filenames_450),len(filenames_850)))\n",
    "    return filenames_450, filenames_850\n",
    "\n",
    "def get_metadata(filenames_450, filenames_850):\n",
    "    \"\"\"Do a shell query using file ids from your manifest file to get desired fields.\n",
    "    Follow this- https://docs.gdc.cancer.gov/API/Users_Guide/Search_and_Retrieval/#example-http-post-request.\n",
    "    I have currently used only non-legacy data from illumina epic arrays.\n",
    "    For 450 arrays, download the biospecimen files and use the sample file for mapping.\n",
    "    \"\"\"\n",
    "    cols = ['cases.0.case_id', 'cases.0.samples.0.sample_id', 'cases.0.samples.0.sample_type', 'file_name']\n",
    "    metaepic = pd.read_csv(r'C:\\Users\\HP\\OneDrive\\Documents\\Bayes\\methluad\\File_metadata.txt', \n",
    "                           header=0, sep = \"\\t\", usecols = cols )\n",
    "    metaepic.columns = ['case_id', 'sample_id', 'sample_type', 'file_name_short']\n",
    "    metaepic = metaepic.replace({'Primary Tumor': 0, 'Solid Tissue Normal': 1, })   \n",
    "    paired = []\n",
    "    for f in filenames_850:\n",
    "        patient = f.split('\\\\')[-1]\n",
    "        paired.append([patient, f])\n",
    "    df_paired = pd.DataFrame(paired)\n",
    "    df_paired.columns = ['file_name_short', 'file_name']\n",
    "    meta850df = metaepic.merge(df_paired, how = 'right', on = 'file_name_short')\n",
    "    print (\"Got metadata for epic data\")\n",
    "     \n",
    "    cols = ['case_id', 'sample_submitter_id', 'sample_id', 'sample_type']\n",
    "    meta450 = pd.read_csv(r'C:\\Users\\HP\\OneDrive\\Documents\\Bayes\\methluad\\sample450.tsv', \n",
    "                          header=0, sep = \"\\t\", usecols = cols)\n",
    "    meta450 = meta450.replace({'Primary Tumor': 0, 'Solid Tissue Normal': 1})\n",
    "    paired = []\n",
    "    for f in filenames_450:\n",
    "        patient = f.split('\\\\')[-1].split('.')[5].split('-')\n",
    "        samplesubmitter = '-'.join(patient[:4])\n",
    "        paired.append([samplesubmitter, f])\n",
    "    df_paired = pd.DataFrame(paired)\n",
    "    df_paired.columns = ['sample_submitter_id', 'file_name']\n",
    "    meta450df = meta450.merge(df_paired, how = 'right', on = 'sample_submitter_id')\n",
    "    print (\"Got metadata for 450 data\")\n",
    "    return meta450df, meta850df\n",
    "\n",
    "def get_methylation_map(annotation_path, methylation_map_path):\n",
    "    \"\"\"Use the 'Infinium MethylationEPIC v1.0 B5 Manifest File'.\n",
    "    Download from https://sapac.support.illumina.com/array/array_kits/infinium-methylationepic-beadchip-kit/downloads.html. \n",
    "    For column heading explanations, see the 'Infinium MethylationEPIC Manifest Column Headings' file. \n",
    "    \"\"\"\n",
    "    \n",
    "    # keep only specified columns\n",
    "    cols = ['IlmnID','CHR','UCSC_RefGene_Name','UCSC_RefGene_Group','GencodeCompV12_NAME',\n",
    "            'GencodeCompV12_Accession', 'GencodeCompV12_Group','Methyl450_Loci']\n",
    "    manifest = pd.read_csv(annotation_path, usecols = cols, skiprows = 7) \n",
    "    # remove control samples, keep only cpg sites # samples starting with 'rs' are blood samples   \n",
    "    manifest = manifest[manifest['IlmnID'].astype(str).str.startswith('cg')]\n",
    "    # remove sites in X chromosome to prevent gender bias and those not mapped to genes\n",
    "    manifest = manifest[manifest.CHR != 'X'] \n",
    "    manifest = manifest[manifest.UCSC_RefGene_Name.isnull() & manifest.GencodeCompV12_NAME.isnull() == False]\n",
    "    \n",
    "    # merge gene mappings from hg19 and ch38, then delete ucsc mappings   \n",
    "    manifest.GencodeCompV12_NAME.fillna(manifest.UCSC_RefGene_Name, inplace = True)\n",
    "    manifest.GencodeCompV12_Group.fillna(manifest.UCSC_RefGene_Group, inplace = True)\n",
    "    manifest = manifest.drop(columns = ['CHR', 'UCSC_RefGene_Name','UCSC_RefGene_Group'])\n",
    "    # keep only cpg sites that are 1500 bp around TSS. Removes about HALF the probes    \n",
    "    manifest_tss = manifest[manifest['GencodeCompV12_Group'].str.contains('TSS200|TSS1500') == True]\n",
    "    manifest_tss.index = range(len(manifest_tss)) # reindex dataframe\n",
    "    manifest_tss.to_csv(r'C:\\Users\\HP\\OneDrive\\Documents\\Bayes\\manifest_tss.csv')\n",
    "    \n",
    "    # keep only genes for each site that are associated with TSS\n",
    "    gene = []\n",
    "    for row in range(len(manifest_tss)):\n",
    "        strings = manifest_tss.GencodeCompV12_NAME[row].split(';')\n",
    "        groups = manifest_tss.GencodeCompV12_Group[row].split(';')\n",
    "        if ((len(set(strings)) == 1) == False): # if all genes are same, keep the first\n",
    "            gene.append(strings[0])\n",
    "        else: # if not, keep the first gene corresponding to TSS group\n",
    "            for elem in range(len(groups)):\n",
    "                if groups[elem].startswith('TSS'):\n",
    "                    gene.append(strings[elem])\n",
    "                    break\n",
    "    manifest_tss['Gene'] = gene\n",
    "    cpg_gene_map = manifest_tss[['IlmnID','Gene']].copy() # final cpg map with cpg sites and target gene\n",
    "    cpg_gene_map.to_csv(methylation_map_path)\n",
    "    print (\"Computed mapping of cpg sites to target genes\")\n",
    "    return cpg_gene_map\n",
    "\n",
    "def filter_pairedsamples(meta450, metaepic):\n",
    "    metaepic_paired = pd.DataFrame(metaepic.groupby(\"case_id\")['file_name'].apply(list)).reset_index(drop=True)['file_name']\n",
    "    metaepic_paired = metaepic_paired[metaepic_paired.map(len) >= 2] # keep only paired samples\n",
    "    meta450_paired = pd.DataFrame(meta450.groupby(\"case_id\")['file_name'].apply(list)).reset_index(drop=True)['file_name']\n",
    "    meta450_paired = meta450_paired[meta450_paired.map(len) >= 2]\n",
    "    print (\"Filtered to get {} 450 and {} 850 matched arrays\".format(len(meta450_paired),len(metaepic_paired)))\n",
    "    return meta450_paired, metaepic_paired\n",
    "        \n",
    "def concat_samples(filenames_450, filenames_850, paired=True):\n",
    "    \"\"\"Create a df of all Illumina samples passed.\n",
    "    Pass paired=True if you only want to keep samples that have both tumor and normal vals.\n",
    "    \"\"\"\n",
    "    dfs1 = []\n",
    "    dfs2 = []\n",
    "    names = [] # TCGA barcode \n",
    "    tumor = [] # tumor starts with 0 and normal with 1\n",
    "    batch = []\n",
    "    \n",
    "    print (\"Getting metadata..\")\n",
    "    meta450, metaepic = get_metadata(filenames_450, filenames_850)\n",
    "    \n",
    "    print (\"Obtaining list of paired samples..\")\n",
    "    if paired==True:\n",
    "        meta450_paired, metaepic_paired = filter_pairedsamples(meta450, metaepic)\n",
    "        filenames_450 = [j for i in meta450_paired for j in i]\n",
    "        filenames_850 = [j for i in metaepic_paired for j in i]\n",
    "        \n",
    "    print (\"Concatenating {} 450 and {} 850 Samples\".format(len(filenames_450),len(filenames_850)))\n",
    "    \n",
    "    for f in filenames_450:\n",
    "        df = pd.read_csv(f, sep = \"\\t\", index_col = 0)\n",
    "        df = pd.Series(df['Beta_value'])\n",
    "        names.append(f)\n",
    "        tumor.append(meta450.loc[meta450['file_name'] == f, 'sample_type'].iloc[0])\n",
    "        batch.append(0)\n",
    "        dfs1.append(df)\n",
    "    totaldf1 = pd.concat(dfs1, axis = 1)\n",
    "    print (\"Concatenated 450 arrays\")\n",
    "    \n",
    "    for f in filenames_850:\n",
    "        df = pd.read_csv(f, sep = \"\\t\", names = ['Composite Element REF','Beta_value'], index_col = 0)\n",
    "        df = pd.Series(df['Beta_value'])\n",
    "        names.append(f)\n",
    "        tumor.append(metaepic.loc[metaepic['file_name'] == f, 'sample_type'].iloc[0])\n",
    "        batch.append(1)\n",
    "        dfs2.append(df)\n",
    "    totaldf2 = pd.concat(dfs2, axis = 1)\n",
    "    print (\"Concatenated 850 arrays\")\n",
    "    \n",
    "    # keep cpg sites in EPIC array - 450 probes not in epic are not considered \n",
    "    totaldf = totaldf2.merge(totaldf1, how = 'left', on = 'Composite Element REF')\n",
    "    totaldf.columns = names\n",
    "    print (\"Concatenated all arrays\")\n",
    "    return totaldf, tumor, batch\n",
    "\n",
    "def plot_pca(dataframe, tumor):\n",
    "\n",
    "    dataframe['sample_type'] = tumor\n",
    "    dataframe.set_index('sample_type', inplace = True)\n",
    "    scaled_data = StandardScaler().fit_transform(dataframe)\n",
    "    pca = PCA(2)\n",
    "    pca = pca.fit(scaled_data) \n",
    "    pca = pca.transform(scaled_data)\n",
    "    per_var = np.round(pca.explained_variance_ratio_ * 100, decimals = 1)\n",
    "    pca_df = pd.DataFrame(pca, columns = labels)\n",
    "    plt.scatter(pca_df.PC1, pca_df.PC2, c = dataframe.index)\n",
    "    plt.xlabel('PC1 - {0}%'.format(per_var[0]))\n",
    "    plt.ylabel('PC2 - {0}%'.format(per_var[1]))\n",
    "    plt.show()\n",
    "    \n",
    "def batch_correct(totaldf, tumor, batch):  \n",
    "    \"\"\"Do batch correction using combat and visualize results using PCA.\n",
    "    \"\"\"\n",
    "    # drop rows that have only NAs\n",
    "    totaldf = totaldf.dropna(how='all')\n",
    "    # add a small value to all 0s instead of removing\n",
    "    totaldf = totaldf.fillna(0.0001)\n",
    "    print (\"Performing batch correction..\")      \n",
    "    # run pyComBat\n",
    "    df_corrected = pycombat(totaldf,batch)\n",
    "\n",
    "    # visualise data clustering before and after combat\n",
    "#     plot_pca(totaldf.transpose(), tumor)\n",
    "#     plt.title('PCA_BEFORE')\n",
    "#     plot_pca(df_corrected.transpose(), tumor)\n",
    "#     plt.title('PCA_AFTER')\n",
    "    return df_corrected\n",
    "    \n",
    "def get_gene_sample_matrix(corrected_df, methylation_map_path):\n",
    "    \"\"\"Create a gene vs sample matrix using batch corrected data.\n",
    "    \"\"\"\n",
    "    cpg_gene_map = pd.read_csv(methylation_map_path, usecols = ['IlmnID','Gene'])\n",
    "    cpg_gene_map.columns = ['Composite Element REF','Gene']\n",
    "    gene_sample_matrix = cpg_gene_map.merge(corrected_df, how = 'left', on = 'Composite Element REF')\n",
    "    gene_sample_matrix = gene_sample_matrix.groupby('Gene', as_index=False).mean()\n",
    "    \n",
    "    # get list of protein coding genes from ensembl\n",
    "    ensembl = pyensembl.EnsemblRelease()\n",
    "    gene_ids = ensembl.gene_ids()\n",
    "    genes = [ensembl.gene_by_id(gene_id) for gene_id in gene_ids]\n",
    "    coding_genes = [gene.gene_name for gene in genes if gene.biotype == 'protein_coding']\n",
    "    coding_genes = list(filter(None, coding_genes)) # remove empty strings\n",
    "    # keep only protein coding genes in gene sample matrix dataframe\n",
    "    gene_sample_matrix = gene_sample_matrix[gene_sample_matrix['Gene'].isin(coding_genes)]\n",
    "    gene_sample_matrix = gene_sample_matrix.set_index('Gene', drop=True)\n",
    "    return gene_sample_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd866c7a",
   "metadata": {},
   "source": [
    "# Main Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "478e1b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = r'C:\\Users\\HP\\Downloads\\infinium-methylationepic-v-1-0-b5-manifest-file.csv'\n",
    "methylation_map = r'C:\\Users\\HP\\OneDrive\\Documents\\Bayes\\methluad\\cpg_gene_mapEPIC.csv'\n",
    "meth_dir = r'C:\\Users\\HP\\Downloads\\methluad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02c57267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 744 arrays\n",
      "Found 466 450k arrays and 211 epic arrays\n",
      "Getting metadata..\n",
      "Got metadata for epic data\n",
      "Got metadata for 450 data\n",
      "Obtaining list of paired samples..\n",
      "Filtered to get 33 450 and 98 850 matched arrays\n",
      "Concatenating 74 450 and 197 850 Samples\n",
      "Concatenated 450 arrays\n",
      "Concatenated 850 arrays\n",
      "Concatenated all arrays\n"
     ]
    }
   ],
   "source": [
    "# get_methylation_map(annotation,methylation_map)\n",
    "filenames_450, filenames_850 = get_filenames(meth_dir)\n",
    "totaldf, tumor, batch = concat_samples(filenames_450,filenames_850, True) # takes time\n",
    "# totaldf.to_csv(r'C:\\Users\\HP\\OneDrive\\Documents\\Bayes\\methluad\\totaldf.tsv.gz', sep='\\t', compression='gzip')\n",
    "# totaldf = pd.read_csv(r'C:\\Users\\HP\\OneDrive\\Documents\\Bayes\\methluad\\totaldf.tsv.gz', sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ac84f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing batch correction..\n",
      "Found 2 batches.\n",
      "Adjusting for 0 covariate(s) or covariate level(s).\n",
      "Standardizing Data across genes.\n",
      "Fitting L/S model and finding priors.\n",
      "Finding parametric adjustments.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15952\\670064323.py:120: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  np.absolute(d_new-d_old)/d_old))  # maximum difference between new and old estimate\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15952\\670064323.py:124: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  adjust = np.asarray([g_new, d_new])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting the Data\n"
     ]
    }
   ],
   "source": [
    "df_corrected = batch_correct(totaldf, tumor, batch)\n",
    "genesampledf = get_gene_sample_matrix(df_corrected, methylation_map)\n",
    "# genesampledf.to_csv(r'C:\\Users\\HP\\OneDrive\\Documents\\Bayes\\methluad\\genesampledf.tsv', sep='\\t', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1c4bca",
   "metadata": {},
   "source": [
    "# Identify differentially methylated genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5b6961a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# get list of top 1000 LUAD genes from disgenet\n",
    "target = pd.read_csv(r'C:\\Users\\HP\\OneDrive\\Documents\\Bayes\\methluad\\C0152013_disease_gda_summary.tsv', \n",
    "                     usecols = ['Gene'], sep = '\\t', nrows = 1000).Gene.tolist()\n",
    "print (len(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f140aa72",
   "metadata": {},
   "source": [
    "1. Using beta value cutoffs of 0.8 and 0.2 in tumor samples only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a188bce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195 7290\n",
      "beta threshold 0.8 and 0.2 for top 1k genes-- 4 , 369\n",
      "['AGER', 'SCT', 'IL37', 'TP53']\n",
      "beta threshold 0.8 and 0.2 for top 500 genes-- 2 , 205\n"
     ]
    }
   ],
   "source": [
    "genesampledf = genesampledf.set_index('Gene', drop=True)\n",
    "genesampledf.columns = tumor\n",
    "tumormean = genesampledf[0].mean(axis=1)\n",
    "hypermeth = tumormean[tumormean.abs()> 0.7]\n",
    "hypometh = tumormean[tumormean.abs()< 0.25]\n",
    "print (len(hypermeth), len(hypometh))\n",
    "\n",
    "common_hyper1k = list(set(hypermeth.index).intersection(set(target)))\n",
    "common_hypo1k = list(set(hypometh.index).intersection(set(target)))\n",
    "print (\"beta threshold 0.8 and 0.2 for top 1k genes--\", len(common_hyper1k),\",\", len(common_hypo1k))\n",
    "print (common_hyper1k)\n",
    "\n",
    "common_hyper500 = list(set(hypermeth.index).intersection(set(target[:500])))\n",
    "common_hypo500 = list(set(hypometh.index).intersection(set(target[:500])))\n",
    "print (\"beta threshold 0.8 and 0.2 for top 500 genes--\", len(common_hyper500),\",\", len(common_hypo500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a485b659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2551 4720\n",
      "beta threshold 0.8 and 0.2 for top 1k genes-- 78 , 269\n",
      "beta threshold 0.8 and 0.2 for top 500 genes-- 38 , 155\n",
      "['CTLA4', 'AURKC', 'HLA-DQA2', 'CEACAM3', 'CLDN18', 'IFNG', 'CLPTM1L', 'ALB', 'TEKT5', 'MSH5-SAPCD1', 'BAZ1A', 'TTN', 'PSG2', 'AGER', 'CSRP3', 'IL17A', 'ULK1', 'MYO18B', 'FGFR3', 'FGF7', 'EGF', 'SLC15A2', 'AATK', 'ANXA1', 'ALOX12B', 'APOM', 'IDO1', 'TP53', 'MS4A1', 'CFB', 'APOA1', 'ARHGAP35', 'SGK2', 'MUC4', 'DDR2', 'CYP21A2', 'TNFSF10', 'ARG1']\n"
     ]
    }
   ],
   "source": [
    "# test the difference before batch correction\n",
    "genesample = get_gene_sample_matrix(totaldf, methylation_map)\n",
    "genesample = genesample.set_index('Gene', drop=True)\n",
    "genesample.columns = tumor\n",
    "tumormean = genesample[0].mean(axis=1)\n",
    "hypermeth = tumormean[tumormean.abs()> 0.7]\n",
    "hypometh = tumormean[tumormean.abs()< 0.25]\n",
    "print (len(hypermeth), len(hypometh))\n",
    "\n",
    "common_hyper1k = list(set(hypermeth.index).intersection(set(target)))\n",
    "common_hypo1k = list(set(hypometh.index).intersection(set(target)))\n",
    "print (\"beta threshold 0.8 and 0.2 for top 1k genes--\", len(common_hyper1k),\",\", len(common_hypo1k))\n",
    "\n",
    "common_hyper500 = list(set(hypermeth.index).intersection(set(target[:500])))\n",
    "common_hypo500 = list(set(hypometh.index).intersection(set(target[:500])))\n",
    "print (\"beta threshold 0.8 and 0.2 for top 500 genes--\", len(common_hyper500),\",\", len(common_hypo500))\n",
    "print (common_hyper500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77cd2ff",
   "metadata": {},
   "source": [
    "2. Using log2fc cutoff of 0.5 between tumor and normal samples collectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bddf721c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 75598 invalid values after computing log2 fold changes\n",
      "Gene\n",
      "AASDH     -0.376397\n",
      "AATF      -0.240675\n",
      "ABHD8     -0.151185\n",
      "ABI3      -0.058600\n",
      "ABRA      -0.121415\n",
      "             ...   \n",
      "ZNF91     -0.755857\n",
      "ZSCAN1    -0.480954\n",
      "ZSCAN18   -0.095060\n",
      "ZSCAN22   -0.066842\n",
      "ZXDC      -0.101638\n",
      "Length: 1720, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:402: RuntimeWarning: invalid value encountered in log2\n",
      "  result = func(self.values, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "normalmean = genesampledf[1].mean(axis=1)\n",
    "\n",
    "# compute log2 fold changes (for each sample, divide by mean normal values. Then compute mean across samples)\n",
    "fc = np.log2(genesampledf[0].divide(normalmean, axis=0))\n",
    "fc_nan = fc.replace([np.inf, -np.inf], np.nan)\n",
    "print (\"Got {} invalid values after computing log2 fold changes\".format(fc_nan.isnull().sum().sum()))\n",
    "ratio = fc_nan.dropna(axis=0) # remove NaN and inf (from division by 0 or 0+eta)\n",
    "# calculate mean of all log2fc samples and save to directory\n",
    "final_mean_meth=ratio.mean(axis=1)\n",
    "final_mean_meth.to_csv(r'C:\\Users\\HP\\OneDrive\\Documents\\Bayes\\methluad\\finalmeth.tsv', sep='\\t')\n",
    "print (final_mean_meth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84312961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 43\n",
      "beta threshold 0.8 and 0.2 for top 1k genes-- 0 , 1\n",
      "[]\n",
      "beta threshold 0.8 and 0.2 for top 500 genes-- 0 , 1\n"
     ]
    }
   ],
   "source": [
    "hypermeth = final_mean_meth[final_mean_meth> 0.5]\n",
    "hypometh = final_mean_meth[final_mean_meth< -0.5]\n",
    "print (len(hypermeth), len(hypometh))\n",
    "\n",
    "common_hyper1k = list(set(hypermeth.index).intersection(set(target)))\n",
    "common_hypo1k = list(set(hypometh.index).intersection(set(target)))\n",
    "print (\"beta threshold 0.8 and 0.2 for top 1k genes--\", len(common_hyper1k),\",\", len(common_hypo1k))\n",
    "print (common_hyper1k)\n",
    "\n",
    "common_hyper500 = list(set(hypermeth.index).intersection(set(target[:500])))\n",
    "common_hypo500 = list(set(hypometh.index).intersection(set(target[:500])))\n",
    "print (\"beta threshold 0.8 and 0.2 for top 500 genes--\", len(common_hyper500),\",\", len(common_hypo500))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7298e26",
   "metadata": {},
   "source": [
    "2. Using log2fc cutoff of 0.5 between tumor and normal samples case-wise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dac4528",
   "metadata": {},
   "source": [
    "# PyCombat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7538b834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the code from pycombat. run in case of too many dependency errors\n",
    "import numpy as np\n",
    "from math import exp\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "import mpmath as mp\n",
    "import pandas as pd\n",
    "\n",
    "def model_matrix(info, intercept=True, drop_first=True):\n",
    "    \"\"\"Creates the model_matrix from batch list\n",
    "    Arguments:\n",
    "        info {list} -- list info with batch or covariates data\n",
    "        intercept {bool} -- boolean for intercept in model matrix\n",
    "    Returns:\n",
    "        matrix -- model matrix generate from batch list\n",
    "    \"\"\"\n",
    "    if not isinstance(info[0], list):\n",
    "        info = [info]\n",
    "    else:\n",
    "        info = info\n",
    "    info_dict = {}\n",
    "    for i in range(len(info)):\n",
    "        info_dict[f\"col{str(i)}\"] = list(map(str,info[i]))\n",
    "    df = pd.get_dummies(pd.DataFrame(info_dict), drop_first=drop_first, dtype=float)\n",
    "    if intercept:\n",
    "        df[\"intercept\"] = 1.0\n",
    "    return df.to_numpy()\n",
    "\n",
    "\n",
    "def all_1(list_of_elements):\n",
    "    \"\"\"checks if all elements in a list are 1s\n",
    "    Arguments:\n",
    "        list_of_elements {list} -- list of elements\n",
    "    Returns:\n",
    "        bool -- True iff all elements of the list are 1s\n",
    "    \"\"\"\n",
    "    return((list_of_elements == 1).all())\n",
    "\n",
    "\n",
    "# aprior and bprior are useful to compute \"hyper-prior values\"\n",
    "# -> prior parameters used to estimate the prior gamma distribution for multiplicative batch effect\n",
    "# aprior - calculates empirical hyper-prior values\n",
    "\n",
    "def compute_prior(prior, gamma_hat, mean_only):\n",
    "    \"\"\"[summary]\n",
    "    Arguments:\n",
    "        prior {char} -- 'a' or 'b' depending of the prior to be calculated\n",
    "        gamma_hat {matrix} -- matrix of additive batch effect\n",
    "        mean_only {bool} -- True iff mean_only selected\n",
    "    Returns:\n",
    "        float -- [the prior calculated (aprior or bprior)\n",
    "    \"\"\"\n",
    "    if mean_only:\n",
    "        return 1\n",
    "    m = np.mean(gamma_hat)\n",
    "    s2 = np.var(gamma_hat)\n",
    "    if prior == 'a':\n",
    "        return (2*s2+m*m)/s2\n",
    "    elif prior == 'b':\n",
    "        return (m*s2+m*m*m)/s2\n",
    "\n",
    "\n",
    "def postmean(g_bar, d_star, t2_n, t2_n_g_hat):\n",
    "    \"\"\"estimates additive batch effect\n",
    "    Arguments:\n",
    "        g_bar {matrix} -- additive batch effect\n",
    "        d_star {matrix} -- multiplicative batch effect\n",
    "        t2_n {matrix} --\n",
    "        t2_n_g_hat {matrix} --\n",
    "    Returns:\n",
    "        matrix -- estimated additive batch effect\n",
    "    \"\"\"\n",
    "    return np.divide(t2_n_g_hat+d_star*g_bar, np.asarray(t2_n+d_star))\n",
    "\n",
    "\n",
    "def postvar(sum2, n, a, b):\n",
    "    \"\"\"estimates multiplicative batch effect\n",
    "    Arguments:\n",
    "        sum2 {vector} --\n",
    "        n {[type]} --\n",
    "        a {float} -- aprior\n",
    "        b {float} -- bprior\n",
    "    Returns:\n",
    "        matrix -- estimated multiplicative batch effect\n",
    "    \"\"\"\n",
    "    return(np.divide((np.multiply(0.5, sum2)+b), (np.multiply(0.5, n)+a-1)))\n",
    "\n",
    "\n",
    "def it_sol(sdat, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001, exit_iteration=10e5):\n",
    "    \"\"\"iterative solution for Empirical Bayesian method\n",
    "    Arguments:\n",
    "        sdat {matrix} --\n",
    "        g_hat {matrix} -- average additive batch effect\n",
    "        d_hat {matrix} -- average multiplicative batch effect\n",
    "        g_bar {matrix} -- additive batch effect\n",
    "        t2 {matrix} --\n",
    "        a {float} -- aprior\n",
    "        b {float} -- bprior\n",
    "    Keyword Arguments:\n",
    "        conv {float} -- convergence criterion (default: {0.0001})\n",
    "        exit_iteration {float} -- maximum number of iterations before exit (default: {10e5})\n",
    "    Returns:\n",
    "        array list -- estimated additive and multiplicative batch effect\n",
    "    \"\"\"\n",
    "\n",
    "    n = [len(i) for i in np.asarray(sdat)]\n",
    "    t2_n = np.multiply(t2, n)\n",
    "    t2_n_g_hat = np.multiply(t2_n, g_hat)\n",
    "    g_old = np.ndarray.copy(g_hat)\n",
    "    d_old = np.ndarray.copy(d_hat)\n",
    "    change = 1\n",
    "    count = 0  # number of steps needed (for diagnostic only)\n",
    "    # convergence criteria, if new-old < conv, then stop\n",
    "    while (change > conv) and (count < exit_iteration):\n",
    "        g_new = postmean(g_bar, d_old, t2_n, t2_n_g_hat)  # updated additive batch effect\n",
    "        sum2 = np.sum(np.asarray(np.square(\n",
    "            sdat-np.outer(g_new[0][0], np.ones(np.ma.size(sdat, axis=1))))), axis=1)\n",
    "        d_new = postvar(sum2, n, a, b)  # updated multiplicative batch effect\n",
    "        change = max(np.amax(np.absolute(g_new-np.asarray(g_old))/np.asarray(g_old)), np.amax(\n",
    "            np.absolute(d_new-d_old)/d_old))  # maximum difference between new and old estimate\n",
    "        g_old = np.ndarray.copy(g_new)  # save value for g\n",
    "        d_old = np.ndarray.copy(d_new)  # save value for d\n",
    "        count += 1\n",
    "    adjust = np.asarray([g_new, d_new])\n",
    "    return(adjust)  # remove parenthesis in returns\n",
    "\n",
    "# int_eprior - Monte Carlo integration function to find nonparametric adjustments\n",
    "# Johnson et al (Biostatistics 2007, supp.mat.) show that we can estimate the multiplicative and additive batch effects with an integral\n",
    "# This integral is numerically computed through Monte Carlo inegration (iterative method)\n",
    "\n",
    "\n",
    "def int_eprior(sdat, g_hat, d_hat, precision):\n",
    "    \"\"\" int_eprior - Monte Carlo integration function to find nonparametric adjustments\n",
    "        Johnson et al (Biostatistics 2007, supp.mat.) show that we can estimate the multiplicative and additive batch effects with an integral\n",
    "        This integral is numerically computed through Monte Carlo inegration (iterative method)\n",
    "    Arguments:\n",
    "        sdat {matrix} -- data matrix\n",
    "        g_hat {matrix} -- average additive batch effect\n",
    "        d_hat {matrix} -- average multiplicative batch effect\n",
    "        precision {float} -- level of precision for precision computing\n",
    "    Returns:\n",
    "        array list -- estimated additive and multiplicative batch effect\n",
    "    \"\"\"\n",
    "    g_star = []\n",
    "    d_star = []\n",
    "    # use this variable to only print error message once if approximation used\n",
    "    test_approximation = 0\n",
    "    for i in range(len(sdat)):\n",
    "        # additive batch effect\n",
    "        g = np.asarray(np.delete(np.transpose(g_hat), i))\n",
    "        # multiplicative batch effect\n",
    "        d = np.asarray(np.delete(np.transpose(d_hat), i))\n",
    "        x = np.asarray(np.transpose(sdat[i]))\n",
    "        n = len(x)\n",
    "        j = [1]*n\n",
    "        dat = np.repeat(x, len(np.transpose(g)), axis=1)\n",
    "        resid2 = np.square(dat-g)\n",
    "        sum2 = np.dot(np.transpose(resid2), j)\n",
    "        # /begin{handling high precision computing}\n",
    "        temp_2d = 2*d\n",
    "        if (precision == None):\n",
    "            LH = np.power(1/(np.pi*temp_2d), n/2)*np.exp(np.negative(sum2)/(temp_2d))\n",
    "\n",
    "        else:  # only if precision parameter informed\n",
    "            # increase the precision of the computing (if negative exponential too close to 0)\n",
    "            mp.dps = precision\n",
    "            buf_exp = np.array(list(map(mp.exp, np.negative(sum2)/(temp_2d))))\n",
    "            buf_pow = np.array(list(map(partial(mp.power, y=n/2), 1/(np.pi*temp_2d))))\n",
    "            #print(buf_exp.dtype, buf_pow.dtype)\n",
    "            LH = buf_pow*buf_exp  # likelihood\n",
    "        # /end{handling high precision computing}\n",
    "        LH = np.nan_to_num(LH)  # corrects NaNs in likelihood\n",
    "        if np.sum(LH) == 0 and test_approximation == 0:\n",
    "            test_approximation = 1  # this message won't appear again\n",
    "            print(\"###\\nValues too small, approximation applied to avoid division by 0.\\nPrecision mode can correct this problem, but increases computation time.\\n###\")\n",
    "\n",
    "        if np.sum(LH) == 0: # correction for LH full of 0.0\n",
    "            LH[LH == 0] = np.exp(-745)\n",
    "            g_star.append(np.sum(g*LH)/np.sum(LH))\n",
    "            d_star.append(np.sum(d*LH)/np.sum(LH))\n",
    "        else:\n",
    "            g_star.append(np.sum(g*LH)/np.sum(LH))\n",
    "            d_star.append(np.sum(d*LH)/np.sum(LH))\n",
    "    adjust = np.asarray([np.asarray(g_star), np.asarray(d_star)])\n",
    "    return(adjust)\n",
    "\n",
    "\n",
    "def param_fun(i, s_data, batches, mean_only, gamma_hat, gamma_bar, delta_hat, t2, a_prior, b_prior):\n",
    "    \"\"\"parametric estimation of batch effects\n",
    "    Arguments:\n",
    "        i {int} -- column index\n",
    "        s_data {matrix} --\n",
    "        batches {list list} -- list of list of batches' elements\n",
    "        mean_only {bool} -- True iff mean_only selected\n",
    "        gamma_hat {matrix} -- average additive batch effect\n",
    "        gamma_bar {matrix} -- estimated additive batch effect\n",
    "        delta_hat {matrix} -- average multiplicative batch effect\n",
    "        t2 {matrix} --\n",
    "        a_prior {float} -- aprior\n",
    "        b_prior {float} -- bprior\n",
    "    Returns:\n",
    "        array list -- estimated adjusted additive and multiplicative batch effect\n",
    "    \"\"\"\n",
    "    if mean_only:  # if mean_only, no need for complex method: batch effect is immediately calculated\n",
    "        t2_n = np.multiply(t2[i], 1)\n",
    "        t2_n_g_hat = np.multiply(t2_n, gamma_hat[i])\n",
    "        gamma_star = postmean(gamma_bar[i], 1, t2_n, t2_n_g_hat)  # additive batch effect\n",
    "        delta_star = [1]*len(s_data)  # multiplicative batch effect\n",
    "    else:  # if not(mean_only) then use it_solve\n",
    "        temp = it_sol(np.transpose(np.transpose(s_data)[\n",
    "                      batches[i]]), gamma_hat[i], delta_hat[i], gamma_bar[i], t2[i], a_prior[i], b_prior[i])\n",
    "        gamma_star = temp[0]  # additive batch effect\n",
    "        delta_star = temp[1]  # multiplicative batch effect\n",
    "    return [gamma_star, delta_star]\n",
    "\n",
    "def nonparam_fun(i, mean_only, delta_hat, s_data, batches, gamma_hat, precision):\n",
    "    \"\"\"non-parametric estimation\n",
    "    Arguments:\n",
    "        i {int} -- column index\n",
    "        mean_only {bool} -- True iff mean_only selected\n",
    "        delta_hat {matrix} -- estimated multiplicative batch effect\n",
    "        s_data {matrix} --\n",
    "        batches {list list} -- list of list of batches' elements\n",
    "        gamma_hat {matrix} -- estimated additive batch effect\n",
    "        precision {float} -- level of precision for precision computing\n",
    "    Returns:\n",
    "        array list -- estimated adjusted additive and multiplicative batch effect\n",
    "    \"\"\"\n",
    "    if mean_only:  # if mean only, change delta_hat to vector of 1s\n",
    "        delta_hat[i] = [1]*len(delta_hat[i])\n",
    "    # use int_eprior for non-parametric estimation\n",
    "    temp = int_eprior(np.transpose(np.transpose(s_data)[\n",
    "                      batches[i]]), gamma_hat[i], delta_hat[i], precision)\n",
    "    return [temp[0], temp[1]]\n",
    "\n",
    "############\n",
    "# pyComBat #\n",
    "############\n",
    "\n",
    "\n",
    "def check_mean_only(mean_only):\n",
    "    \"\"\"checks mean_only option\n",
    "    Arguments:\n",
    "        mean_only {boolean} -- user's choice about mean_only\n",
    "    Returns:\n",
    "        ()\n",
    "    \"\"\"\n",
    "    if mean_only == True:\n",
    "        print(\"Using mean only version\")\n",
    "\n",
    "\n",
    "def define_batchmod(batch):\n",
    "    \"\"\"generates model matrix\n",
    "    Arguments:\n",
    "        batch {list} -- list of batch id\n",
    "    Returns:\n",
    "        batchmod {matrix} -- model matrix for batches\n",
    "    \"\"\"\n",
    "    batchmod = model_matrix(list(batch), intercept=False, drop_first=False)\n",
    "    return(batchmod)\n",
    "\n",
    "\n",
    "def check_ref_batch(ref_batch, batch, batchmod):\n",
    "    \"\"\"check ref_batch option and treat it if needed\n",
    "    Arguments:\n",
    "        ref_batch {int} -- the reference batch\n",
    "        batch {list} -- list of batch id\n",
    "        batchmod {matrix} -- model matrix related to batches\n",
    "    Returns:\n",
    "        ref {int list} -- the corresponding positions of the reference batch in the batch list\n",
    "        batchmod {matrix} -- updated model matrix related to batches, with reference\n",
    "    \"\"\"\n",
    "    if ref_batch is not None:\n",
    "        if ref_batch not in batch:\n",
    "            print(\"Reference level ref.batch must be one of the levels of batch.\")\n",
    "            exit(0)\n",
    "        print(\"Using batch \"+str(ref_batch) +\n",
    "              \" as a reference batch.\")\n",
    "        # ref keeps in memory the columns concerned by the reference batch\n",
    "        ref = np.where(np.unique(batch) == ref_batch)[0][0]\n",
    "        # updates batchmod with reference\n",
    "        batchmod[:,ref] = 1\n",
    "    else:\n",
    "        ref = None  # default settings\n",
    "    return(ref, batchmod)\n",
    "\n",
    "\n",
    "def treat_batches(batch):\n",
    "    \"\"\"treat batches\n",
    "    Arguments:\n",
    "        batch {list} -- batch list\n",
    "    Returns:\n",
    "        n_batch {int} -- number of batches\n",
    "        batches {int list} -- list of unique batches\n",
    "        n_batches {int list} -- list of batches lengths\n",
    "        n_array {int} -- total size of dataset\n",
    "    \"\"\"\n",
    "    batch = pd.Series(batch)\n",
    "    n_batch = len(np.unique(batch))  # number of batches\n",
    "    print(\"Found \"+str(n_batch)+\" batches.\")\n",
    "    batches = []  # list of lists, contains the list of position for each batch\n",
    "    for i in range(n_batch):\n",
    "        batches.append(np.where(batch == np.unique(batch)[i])[0].astype(np.int32))\n",
    "    n_batches = list(map(len, batches))\n",
    "    if 1 in n_batches:\n",
    "        #mean_only = True  # no variance if only one sample in a batch - mean_only has to be used\n",
    "        print(\"\\nOne batch has only one sample, try setting mean_only=True.\\n\")\n",
    "    n_array = sum(n_batches)\n",
    "    return(n_batch, batches, n_batches, n_array)\n",
    "\n",
    "\n",
    "def treat_covariates(batchmod, mod, ref, n_batch):\n",
    "    \"\"\"treat covariates\n",
    "    Arguments:\n",
    "        batchmod {matrix} -- model matrix for batch\n",
    "        mod {matrix} -- model matrix for other covariates\n",
    "        ref {int} -- reference batch\n",
    "        n_batch {int} -- number of batches\n",
    "    Returns:\n",
    "        check {bool list} -- a list characterising all covariates\n",
    "        design {matrix} -- model matrix for all covariates, including batch\n",
    "    \"\"\"\n",
    "    # design matrix for sample conditions\n",
    "    if mod == []:\n",
    "        design = batchmod\n",
    "    else:\n",
    "        mod_matrix = model_matrix(mod, intercept=True)\n",
    "        design = np.concatenate((batchmod, mod_matrix), axis=1)\n",
    "    check = list(map(all_1, np.transpose(design)))\n",
    "    if ref is not None:  # if ref\n",
    "        check[ref] = False  # the reference in not considered as a covariate\n",
    "    design = design[:, ~np.array(check)]\n",
    "    design = np.transpose(design)\n",
    "\n",
    "    print(\"Adjusting for \"+str(len(design)-len(np.transpose(batchmod))) +\n",
    "          \" covariate(s) or covariate level(s).\")\n",
    "\n",
    "    # if matrix cannot be invertible, different cases\n",
    "    if np.linalg.matrix_rank(design) < len(design):\n",
    "        if len(design) == n_batch + 1:  # case 1: covariate confunded with a batch\n",
    "            print(\n",
    "                \"The covariate is confunded with batch. Remove the covariate and rerun pyComBat.\")\n",
    "            exit(0)\n",
    "        if len(design) > n_batch + 1:  # case 2: multiple covariates confunded with a batch\n",
    "            if np.linalg.matrix_rank(np.transpose(design)[:n_batch]) < len(design):\n",
    "                print(\n",
    "                    \"The covariates are confounded! Please remove one or more of the covariates so the design is not confounded.\")\n",
    "                exit(0)\n",
    "            else:  # case 3: at least a covariate confunded with a batch\n",
    "                print(\n",
    "                    \"At least one covariate is confounded with batch. Please remove confounded covariates and rerun pyComBat\")\n",
    "                exit(0)\n",
    "    return(design)\n",
    "\n",
    "\n",
    "def check_NAs(dat):\n",
    "    \"\"\"check if NaNs - in theory, we construct the data without NAs\n",
    "    Arguments:\n",
    "        dat {matrix} -- the data matrix\n",
    "    Returns:\n",
    "        NAs {bool} -- boolean characterising the presence of NaNs in the data matrix\n",
    "    \"\"\"\n",
    "    # NAs = True in (np.isnan(dat))\n",
    "    NAs = np.isnan(np.sum(dat))  # Check if NaN exists\n",
    "    if NAs:\n",
    "        print(\"Found missing data values. Please remove all missing values before proceeding with pyComBat.\")\n",
    "    return(NAs)\n",
    "\n",
    "\n",
    "def calculate_mean_var(design, batches, ref, dat, NAs, ref_batch, n_batches, n_batch, n_array):\n",
    "    \"\"\" calculates the Normalisation factors\n",
    "    Arguments:\n",
    "        design {matrix} -- model matrix for all covariates\n",
    "        batches {int list} -- list of unique batches\n",
    "        dat {matrix} -- data matrix\n",
    "        NAs {bool} -- presence of NaNs in the data matrix\n",
    "        ref_batch {int} -- reference batch\n",
    "        n_batches {int list} -- list of batches lengths\n",
    "        n_array {int} -- total size of dataset\n",
    "    Returns:\n",
    "        B_hat {matrix} -- regression coefficients corresponding to the design matrix\n",
    "        grand_mean {matrix} -- Mean for each gene and each batch\n",
    "        var_pooled {matrix} -- Variance for each gene and each batch\n",
    "    \"\"\"\n",
    "    print(\"Standardizing Data across genes.\")\n",
    "    if not(NAs):  # NAs not supported\n",
    "        # B_hat is the vector of regression coefficients corresponding to the design matrix\n",
    "        B_hat = np.linalg.solve(np.dot(design, np.transpose(\n",
    "            design)), np.dot(design, np.transpose(dat)))\n",
    "\n",
    "    # Calculates the general mean\n",
    "    if ref_batch is not None:\n",
    "        grand_mean = np.transpose(B_hat[ref])\n",
    "    else:\n",
    "        grand_mean = np.dot(np.transpose(\n",
    "            [i / n_array for i in n_batches]), B_hat[0:n_batch])\n",
    "    # Calculates the general variance\n",
    "    if not NAs:  # NAs not supported\n",
    "        if ref_batch is not None:  # depending on ref batch\n",
    "            ref_dat = np.transpose(np.transpose(dat)[batches[ref]])\n",
    "            var_pooled = np.dot(np.square(ref_dat - np.transpose(np.dot(np.transpose(\n",
    "                design)[batches[ref]], B_hat))), [1/n_batches[ref]]*n_batches[ref])\n",
    "        else:\n",
    "            var_pooled = np.dot(np.square(\n",
    "                dat - np.transpose(np.dot(np.transpose(design), B_hat))), [1/n_array]*n_array)\n",
    "\n",
    "    return(B_hat, grand_mean, var_pooled)\n",
    "\n",
    "\n",
    "def calculate_stand_mean(grand_mean, n_array, design, n_batch, B_hat):\n",
    "    \"\"\" transform the format of the mean for substraction\n",
    "    Arguments:\n",
    "        grand_mean {matrix} -- Mean for each gene and each batch\n",
    "        n_array {int} -- total size of dataset\n",
    "        design {[type]} -- design matrix for all covariates including batch\n",
    "        n_batch {int} -- number of batches\n",
    "        B_hat {matrix} -- regression coefficients corresponding to the design matrix\n",
    "    Returns:\n",
    "        stand_mean {matrix} -- standardised mean\n",
    "    \"\"\"\n",
    "    stand_mean = np.dot(np.transpose(np.mat(grand_mean)), np.mat([1]*n_array))\n",
    "    # corrects the mean with design matrix information\n",
    "    if design is not None:\n",
    "        tmp = np.ndarray.copy(design)\n",
    "        tmp[0:n_batch] = 0\n",
    "        stand_mean = stand_mean + \\\n",
    "            np.transpose(np.dot(np.transpose(tmp), B_hat))\n",
    "    return(stand_mean)\n",
    "\n",
    "\n",
    "def standardise_data(dat, stand_mean, var_pooled, n_array):\n",
    "    \"\"\"standardise the data: substract mean and divide by variance\n",
    "    Arguments:\n",
    "        dat {matrix} -- data matrix\n",
    "        stand_mean {matrix} -- standardised mean\n",
    "        var_pooled {matrix} -- Variance for each gene and each batch\n",
    "        n_array {int} -- total size of dataset\n",
    "    Returns:\n",
    "        s_data {matrix} -- standardised data matrix\n",
    "    \"\"\"\n",
    "    s_data = (dat - stand_mean) / \\\n",
    "        np.dot(np.transpose(np.mat(np.sqrt(var_pooled))), np.mat([1]*n_array))\n",
    "    return(s_data)\n",
    "\n",
    "\n",
    "def fit_model(design, n_batch, s_data, batches, mean_only, par_prior, precision, ref_batch, ref, NAs):\n",
    "    print(\"Fitting L/S model and finding priors.\")\n",
    "\n",
    "    # fraction of design matrix related to batches\n",
    "    batch_design = design[0:n_batch]\n",
    "\n",
    "    if not NAs:  # CF SUPRA FOR NAs\n",
    "        # gamma_hat is the vector of additive batch effect\n",
    "        gamma_hat = np.linalg.solve(np.dot(batch_design, np.transpose(batch_design)),\n",
    "                                    np.dot(batch_design, np.transpose(s_data)))\n",
    "\n",
    "    delta_hat = []  # delta_hat is the vector of estimated multiplicative batch effect\n",
    "\n",
    "    if (mean_only):\n",
    "        # no variance if mean_only == True\n",
    "        delta_hat = [np.asarray([1]*len(s_data))] * len(batches)\n",
    "    else:\n",
    "        for i in batches:  # feed incrementally delta_hat\n",
    "            list_map = np.transpose(np.transpose(s_data)[i]).var(\n",
    "                axis=1)  # variance for each row\n",
    "            delta_hat.append(np.squeeze(np.asarray(list_map)))\n",
    "\n",
    "    gamma_bar = list(map(np.mean, gamma_hat))  # vector of means for gamma_hat\n",
    "    t2 = list(map(np.var, gamma_hat))  # vector of variances for gamma_hat\n",
    "\n",
    "    # calculates hyper priors for gamma (additive batch effect)\n",
    "    a_prior = list(\n",
    "        map(partial(compute_prior, 'a', mean_only=mean_only), delta_hat))\n",
    "    b_prior = list(\n",
    "        map(partial(compute_prior, 'b', mean_only=mean_only), delta_hat))\n",
    "\n",
    "    # initialise gamma and delta for parameters estimation\n",
    "    gamma_star = np.empty((n_batch, len(s_data)))\n",
    "    delta_star = np.empty((n_batch, len(s_data)))\n",
    "\n",
    "    if par_prior:\n",
    "        # use param_fun function for parametric adjustments (cf. function definition)\n",
    "        print(\"Finding parametric adjustments.\")\n",
    "        results = list(map(partial(param_fun,\n",
    "                                   s_data=s_data,\n",
    "                                   batches=batches,\n",
    "                                   mean_only=mean_only,\n",
    "                                   gamma_hat=gamma_hat,\n",
    "                                   gamma_bar=gamma_bar,\n",
    "                                   delta_hat=delta_hat,\n",
    "                                   t2=t2,\n",
    "                                   a_prior=a_prior,\n",
    "                                   b_prior=b_prior), range(n_batch)))\n",
    "    else:\n",
    "        # use nonparam_fun for non-parametric adjustments (cf. function definition)\n",
    "        print(\"Finding nonparametric adjustments\")\n",
    "        results = list(map(partial(nonparam_fun, mean_only=mean_only, delta_hat=delta_hat,\n",
    "                                   s_data=s_data, batches=batches, gamma_hat=gamma_hat, precision=precision), range(n_batch)))\n",
    "\n",
    "    for i in range(n_batch):  # store the results in gamma/delta_star\n",
    "        results_i = results[i]\n",
    "        gamma_star[i], delta_star[i] = results_i[0], results_i[1]\n",
    "\n",
    "    # update if reference batch (the reference batch is not supposed to be modified)\n",
    "    if ref_batch:\n",
    "        len_gamma_star_ref = len(gamma_star[ref])\n",
    "        gamma_star[ref] = [0] * len_gamma_star_ref\n",
    "        delta_star[ref] = [1] * len_gamma_star_ref\n",
    "\n",
    "    return(gamma_star, delta_star, batch_design)\n",
    "\n",
    "\n",
    "def adjust_data(s_data, gamma_star, delta_star, batch_design, n_batches, var_pooled, stand_mean, n_array, ref_batch, ref, batches, dat):\n",
    "    \"\"\"Adjust the data -- corrects for estimated batch effects\n",
    "    Arguments:\n",
    "        s_data {matrix} -- standardised data matrix\n",
    "        gamma_star {matrix} -- estimated additive batch effect\n",
    "        delta_star {matrix} -- estimated multiplicative batch effect\n",
    "        batch_design {matrix} -- information about batches in design matrix\n",
    "        n_batches {int list} -- list of batches lengths\n",
    "        stand_mean {matrix} -- standardised mean\n",
    "        var_pooled {matrix} -- Variance for each gene and each batch\n",
    "        n_array {int} -- total size of dataset\n",
    "        ref_batch {int} -- reference batch\n",
    "        ref {int list} -- the corresponding positions of the reference batch in the batch list\n",
    "        batches {int list} -- list of unique batches\n",
    "        dat\n",
    "    Returns:\n",
    "        bayes_data [matrix] -- data adjusted for correction of batch effects\n",
    "    \"\"\"\n",
    "    # Now we adjust the data:\n",
    "    # 1. substract additive batch effect (gamma_star)\n",
    "    # 2. divide by multiplicative batch effect (delta_star)\n",
    "    print(\"Adjusting the Data\")\n",
    "    bayes_data = np.transpose(s_data)\n",
    "    j = 0\n",
    "    for i in batches:  # for each batch, specific correction\n",
    "        bayes_data[i] = (bayes_data[i] - np.dot(np.transpose(batch_design)[i], gamma_star)) / \\\n",
    "            np.transpose(\n",
    "                np.outer(np.sqrt(delta_star[j]), np.asarray([1]*n_batches[j])))\n",
    "        j += 1\n",
    "\n",
    "    # renormalise the data after correction:\n",
    "    # 1. multiply by variance\n",
    "    # 2. add mean\n",
    "    bayes_data = np.multiply(np.transpose(bayes_data), np.outer(\n",
    "        np.sqrt(var_pooled), np.asarray([1]*n_array))) + stand_mean\n",
    "\n",
    "    # correction for reference batch\n",
    "    if ref_batch:\n",
    "        bayes_data[batches[ref]] = dat[batches[ref]]\n",
    "\n",
    "    # returns the data corrected for batch effects\n",
    "    return bayes_data\n",
    "\n",
    "\n",
    "def pycombat(data, batch, mod=[], par_prior=True, prior_plots=False, mean_only=False, ref_batch=None, precision=None, **kwargs):\n",
    "    \"\"\"Corrects batch effect in microarray expression data. Takes an gene expression file and a list of known batches corresponding to each sample.\n",
    "    Arguments:\n",
    "        data {matrix} -- The expression matrix (dataframe). It contains the information about the gene expression (rows) for each sample (columns).\n",
    "        batch {list} -- List of batch indexes. The batch list describes the batch for each sample. The batches list has as many elements as the number of columns in the expression matrix.\n",
    "    Keyword Arguments:\n",
    "        mod {list} -- List (or list of lists) of covariate(s) indexes. The mod list describes the covariate(s) for each sample. Each mod list has as many elements as the number of columns in the expression matrix (default: {[]}).\n",
    "        par_prior {bool} -- False for non-parametric estimation of batch effects (default: {True}).\n",
    "        prior_plots {bool} -- True if requires to plot the priors (default: {False} -- Not implemented yet!).\n",
    "        mean_only {bool} -- True iff just adjusting the means and not individual batch effects (default: {False}).\n",
    "        ref_batch {int} -- reference batch selected (default: {None}).\n",
    "        precision {float} -- level of precision for precision computing (default: {None}).\n",
    "    Returns:\n",
    "        bayes_data_df -- The expression dataframe adjusted for batch effects.\n",
    "    \"\"\"\n",
    "\n",
    "    list_samples = data.columns\n",
    "    list_genes = data.index\n",
    "    dat = data.values\n",
    "\n",
    "    check_mean_only(mean_only)\n",
    "\n",
    "    batchmod = define_batchmod(batch)\n",
    "    ref, batchmod = check_ref_batch(ref_batch, batch, batchmod)\n",
    "    n_batch, batches, n_batches, n_array = treat_batches(batch)\n",
    "    design = treat_covariates(batchmod, mod, ref, n_batch)\n",
    "    NAs = check_NAs(dat)\n",
    "    if not(NAs):\n",
    "        B_hat, grand_mean, var_pooled = calculate_mean_var(\n",
    "            design, batches, ref, dat, NAs, ref_batch, n_batches, n_batch, n_array)\n",
    "        stand_mean = calculate_stand_mean(\n",
    "            grand_mean, n_array, design, n_batch, B_hat)\n",
    "        s_data = standardise_data(dat, stand_mean, var_pooled, n_array)\n",
    "        gamma_star, delta_star, batch_design = fit_model(\n",
    "            design, n_batch, s_data, batches, mean_only, par_prior, precision, ref_batch, ref, NAs)\n",
    "        bayes_data = adjust_data(s_data, gamma_star, delta_star, batch_design,\n",
    "                                n_batches, var_pooled, stand_mean, n_array, ref_batch, ref, batches, dat)\n",
    "\n",
    "        bayes_data_df = pd.DataFrame(bayes_data,\n",
    "                    columns = list_samples,\n",
    "                    index = list_genes)\n",
    "\n",
    "        return(bayes_data_df)\n",
    "    else:\n",
    "        raise ValueError(\"NaN value is not accepted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
